{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcc133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_resume_cfg_single_source.py\n",
    "import re, os, json, time, argparse, yaml, requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "# ===== normalizers (tiny) =====\n",
    "# Normalize text: remove extra spaces, lowercase, strip edges\n",
    "_norm = lambda s: re.sub(r\"\\s+\",\" \",(s or \"\").strip().lower())\n",
    "\n",
    "# Normalize email: strip whitespace and lowercase\n",
    "_norm_email = lambda s: (s or \"\").strip().lower()\n",
    "\n",
    "# Normalize skills list: clean each skill, remove empties, return as set\n",
    "_norm_skills = lambda xs: { _norm(x) for x in (xs or []) if _norm(x) }\n",
    "\n",
    "# ===== load config ONCE (single source of truth) =====\n",
    "def load_cfg(config_path: str) -> Dict:\n",
    "    \"\"\"Load and validate YAML configuration for LLM-based resume extraction.\"\"\"\n",
    "    # Get config file path and parent directory for relative path resolution\n",
    "    cfgp = Path(config_path); base = cfgp.parent\n",
    "    \n",
    "    # Load YAML configuration safely\n",
    "    cfg = yaml.safe_load(cfgp.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Validate required top-level configuration fields\n",
    "    for k in [\"input_dir\",\"output_dir\",\"ground_truth_path\",\"llm\"]:\n",
    "        assert k in cfg, f\"Config missing '{k}'\"\n",
    "    \n",
    "    # Validate required LLM configuration fields\n",
    "    for k in [\"api_key\",\"endpoint\",\"model\",\"temperature\",\"timeout_s\",\n",
    "              \"max_retries\",\"min_call_interval_s\",\"prompt_mode\",\"max_chars\",\"max_tokens\"]:\n",
    "        assert k in cfg[\"llm\"], f\"Config.llm missing '{k}'\"\n",
    "\n",
    "    # Resolve all paths relative to the config file location\n",
    "    paths = {\n",
    "        \"input_dir\": (base / cfg[\"input_dir\"]).resolve(),                    # Directory with resume files\n",
    "        \"output_dir\": (base / cfg[\"output_dir\"]).resolve(),                  # Output directory for results\n",
    "        \"ground_truth_path\": (base / cfg[\"ground_truth_path\"]).resolve(),    # Ground truth JSON file\n",
    "        \"aggregate_path\": (base / cfg.get(\"aggregate_path\")).resolve() if cfg.get(\"aggregate_path\") else None,  # Optional aggregate output file\n",
    "    }\n",
    "    \n",
    "    # Extract optional settings with defaults\n",
    "    options = {\n",
    "        \"exts\": set(cfg.get(\"exts\", [\".pdf\",\".docx\",\".txt\"])),  # Supported file extensions (includes .txt for LLM version)\n",
    "        \"print_each\": bool(cfg.get(\"print_each_resume\", False)), # Print status for each processed file\n",
    "    }\n",
    "    \n",
    "    # Pass LLM config as-is without any defaults (all required fields validated above)\n",
    "    llm = cfg[\"llm\"]\n",
    "\n",
    "    return {\"paths\": paths, \"options\": options, \"llm\": llm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf0731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Class 1: LLM extractor (reads everything from cfg) =====\n",
    "class LLMResumeExtractor:\n",
    "    \"\"\"LLM-based resume parser that uses API calls to extract structured data.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Dict):\n",
    "        \"\"\"Initialize with LLM configuration and rate limiting.\"\"\"\n",
    "        self.llm = cfg[\"llm\"]  # Store all LLM settings\n",
    "        self.max_chars = int(self.llm[\"max_chars\"])  # Text truncation limit for API calls\n",
    "        self._last_call_at = 0.0  # Track last API call for rate limiting\n",
    "\n",
    "    def _read_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract plain text from .txt, .docx, or .pdf files.\"\"\"\n",
    "        p = Path(file_path); suf = p.suffix.lower()\n",
    "        \n",
    "        # Handle plain text files\n",
    "        if suf == \".txt\":\n",
    "            try: \n",
    "                return p.read_text(encoding=\"utf-8\", errors=\"ignore\", newline=\" \")\n",
    "            except: \n",
    "                return \"\"\n",
    "        \n",
    "        # Handle Word documents\n",
    "        if suf == \".docx\":\n",
    "            try:\n",
    "                from docx import Document  # Lazy import\n",
    "                return \"\\n\".join(para.text for para in Document(str(p)).paragraphs)\n",
    "            except: \n",
    "                return \"\"\n",
    "        \n",
    "        # Handle PDF documents\n",
    "        if suf == \".pdf\":\n",
    "            try:\n",
    "                import pdfplumber  # Lazy import\n",
    "                with pdfplumber.open(str(p)) as pdf:\n",
    "                    return \"\\n\".join((pg.extract_text() or \"\") for pg in pdf.pages)\n",
    "            except: \n",
    "                return \"\"\n",
    "        \n",
    "        return \"\"  # Unsupported file type\n",
    "\n",
    "    @staticmethod\n",
    "    def _sys() -> str:\n",
    "        \"\"\"Generate system prompt with strict JSON format requirements.\"\"\"\n",
    "        return ('Extract ONLY what exists. Return STRICT JSON: '\n",
    "                '{\"name\":\"string\",\"email\":\"string\",\"skills\":[\"string\",...]}. '\n",
    "                'No prose. Skills lowercase, dedup. Empty when missing.')\n",
    "\n",
    "    def _usr(self, text: str) -> str:\n",
    "        \"\"\"Generate user prompt with optional few-shot examples.\"\"\"\n",
    "        # Truncate text to stay within API limits\n",
    "        text = (text or \"\")[: self.max_chars]\n",
    "        \n",
    "        # Use few-shot prompting if configured\n",
    "        if self.llm[\"prompt_mode\"] == \"few\":\n",
    "            return (\"Example A (output JSON only):\\n\"\n",
    "                    \"INPUT: 'Jane Doe\\\\nEmail: jane@ex.com\\\\nSkills: Python, SQL'\\\\n\"\n",
    "                    'OUTPUT: {\"name\":\"Jane Doe\",\"email\":\"jane@ex.com\",\"skills\":[\"python\",\"sql\"]}\\n'\n",
    "                    \"Example B (missing email):\\n\"\n",
    "                    \"INPUT: 'John Smith\\\\nSkills: Docker, Kubernetes'\\\\n\"\n",
    "                    'OUTPUT: {\"name\":\"John Smith\",\"email\":\"\",\"skills\":[\"docker\",\"kubernetes\"]}\\n\\n'\n",
    "                    f\"Now extract JSON from RESUME:\\n{text}\")\n",
    "        \n",
    "        # Single-shot prompting (simpler, less context)\n",
    "        return f\"Extract JSON with that schema from this RESUME:\\n{text}\"\n",
    "\n",
    "    def _coerce(self, s: str) -> Optional[Dict]:\n",
    "        \"\"\"Clean and parse LLM response into structured dictionary.\"\"\"\n",
    "        if not s: return None\n",
    "        \n",
    "        # Remove markdown code blocks\n",
    "        s = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.I|re.S).strip()\n",
    "        \n",
    "        # Extract JSON object from response\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S); s = m.group(0) if m else s\n",
    "        \n",
    "        # Fix common quote character issues\n",
    "        s = s.replace(\"\\u201c\",'\"').replace(\"\\u201d\",'\"').replace(\"'\",'\"')\n",
    "        \n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "            if not isinstance(obj, dict): return None\n",
    "            \n",
    "            # Ensure string types and clean data\n",
    "            name, email = str(obj.get(\"name\",\"\")), str(obj.get(\"email\",\"\"))\n",
    "            skills = [str(x) for x in (obj.get(\"skills\") or []) if str(x).strip()]\n",
    "            \n",
    "            return {\"name\": name, \"email\": email, \"skills\": skills}\n",
    "        except: \n",
    "            return None\n",
    "\n",
    "    def _call(self, messages: List[Dict[str,str]]) -> Optional[Dict]:\n",
    "        \"\"\"Make rate-limited API call to LLM service.\"\"\"\n",
    "        # Enforce minimum interval between API calls\n",
    "        elapsed = time.time() - self._last_call_at\n",
    "        wait = float(self.llm[\"min_call_interval_s\"]) - elapsed\n",
    "        if wait > 0: time.sleep(wait)\n",
    "        \n",
    "        try:\n",
    "            # Make POST request to LLM API\n",
    "            r = requests.post(\n",
    "                self.llm[\"endpoint\"],\n",
    "                headers={\"Authorization\": f\"Bearer {self.llm['api_key']}\", \"Content-Type\":\"application/json\"},\n",
    "                json={\n",
    "                    \"model\": self.llm[\"model\"],\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": float(self.llm[\"temperature\"]),\n",
    "                    \"max_tokens\": int(self.llm[\"max_tokens\"]),\n",
    "                },\n",
    "                timeout=int(self.llm[\"timeout_s\"]),\n",
    "            )\n",
    "            \n",
    "            # Update rate limiting tracker\n",
    "            self._last_call_at = time.time()\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            # Extract response content and parse JSON\n",
    "            txt = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return self._coerce(txt)\n",
    "        except: \n",
    "            return None\n",
    "\n",
    "    def parse_one(self, file_path: str) -> Dict:\n",
    "        \"\"\"Parse single resume file with retry logic and fallbacks.\"\"\"\n",
    "        text = self._read_text(file_path)\n",
    "        \n",
    "        # Prepare messages for LLM API call\n",
    "        msgs = [{\"role\":\"system\",\"content\":self._sys()},\n",
    "                {\"role\":\"user\",\"content\":self._usr(text)}]\n",
    "        \n",
    "        obj = None\n",
    "        # Retry with configured max attempts\n",
    "        for a in range(int(self.llm[\"max_retries\"])):\n",
    "            obj = self._call(msgs)\n",
    "            if obj: break\n",
    "            \n",
    "            # Last attempt: switch to simpler prompt to reduce drift\n",
    "            if a == int(self.llm[\"max_retries\"]) - 1 and self.llm[\"prompt_mode\"] == \"few\":\n",
    "                self.llm[\"prompt_mode\"] = \"single\"\n",
    "                msgs[1][\"content\"] = self._usr(text)\n",
    "\n",
    "        # Use rule-based fallbacks if LLM extraction fails\n",
    "        if not obj:\n",
    "            obj = {\"name\":\"\", \"email\":\"\", \"skills\":[]}\n",
    "            \n",
    "            # Fallback email extraction using regex\n",
    "            m = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text or \"\")\n",
    "            if m: obj[\"email\"] = m.group(0)\n",
    "            \n",
    "            # Fallback name extraction using heuristics\n",
    "            for line in (text or \"\").splitlines():\n",
    "                line = line.strip()\n",
    "                if 3<=len(line)<=80 and 2<=len(line.split())<=5 and re.fullmatch(r\"[A-Za-zÀ-ÖØ-öø-ÿ.'\\- ]+\", line):\n",
    "                    obj[\"name\"] = line; break\n",
    "        \n",
    "        # Normalize and sort skills for consistent output\n",
    "        obj[\"skills\"] = sorted(_norm_skills(obj.get(\"skills\")))\n",
    "        \n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36cc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Class 2: Evaluator/writer (uses cfg) =====\n",
    "class ResumeEvaluator:\n",
    "    \"\"\"Evaluates LLM extraction results against ground truth and writes output files.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Dict):\n",
    "        \"\"\"Initialize evaluator with paths, ground truth data, and metric counters.\"\"\"\n",
    "        self.paths = cfg[\"paths\"]; self.options = cfg[\"options\"]\n",
    "        \n",
    "        # Load ground truth data (filename -> expected results mapping)\n",
    "        self.gt = json.loads(self.paths[\"ground_truth_path\"].read_text(encoding=\"utf-8\"))\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        self.paths[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize evaluation metric counters\n",
    "        self.n_name=self.n_email=self.ok_name=self.ok_email=0  # Name/email accuracy counters\n",
    "        self.tp=self.fp=self.fn=0  # True positive, false positive, false negative for skills\n",
    "        \n",
    "        # Storage for optional aggregate output file\n",
    "        self.aggregate = {}\n",
    "\n",
    "    def _cmp_missing(self, ex: Dict, gt: Dict):\n",
    "        \"\"\"Compare extracted data with ground truth and identify missing/incorrect fields.\"\"\"\n",
    "        miss={}\n",
    "        \n",
    "        # Check name match (normalized comparison)\n",
    "        if _norm(ex.get(\"name\")) != _norm(gt.get(\"name\")): \n",
    "            miss[\"name\"]=gt.get(\"name\")\n",
    "        \n",
    "        # Check email match (normalized comparison)\n",
    "        if _norm_email(ex.get(\"email\")) != _norm_email(gt.get(\"email\")): \n",
    "            miss[\"email\"]=gt.get(\"email\")\n",
    "        \n",
    "        # Compare skill sets and find missing ones\n",
    "        exs, gts = _norm_skills(ex.get(\"skills\")), _norm_skills(gt.get(\"skills\"))\n",
    "        lack = sorted(gts - exs)  # Skills in ground truth but not extracted\n",
    "        ex_extra = sorted(exs - gts)  # Skills extracted but not in ground truth (for debugging)\n",
    "        if lack: miss[\"skills\"]=lack\n",
    "        \n",
    "        return miss, exs, gts\n",
    "\n",
    "    def _update(self, exs:Set[str], gts:Set[str], name_ok:bool, email_ok:bool):\n",
    "        \"\"\"Update evaluation metrics based on comparison results.\"\"\"\n",
    "        # Update name/email counters\n",
    "        self.n_name+=1; self.n_email+=1\n",
    "        if name_ok: self.ok_name+=1\n",
    "        if email_ok: self.ok_email+=1\n",
    "        \n",
    "        # Update skills metrics (micro-averaged precision/recall)\n",
    "        inter=len(exs & gts)  # Correctly identified skills\n",
    "        self.tp+=inter  # True positives\n",
    "        self.fp+=max(0,len(exs)-inter)  # False positives (extracted but wrong)\n",
    "        self.fn+=max(0,len(gts)-inter)  # False negatives (missed from ground truth)\n",
    "\n",
    "    def process_one(self, file_path: str, extracted: Dict) -> bool:\n",
    "        \"\"\"Process one resume: evaluate, write individual output, update aggregate.\"\"\"\n",
    "        fname = Path(file_path).name\n",
    "        gt = self.gt.get(fname)\n",
    "        \n",
    "        # Skip files not in ground truth (can't evaluate)\n",
    "        if gt is None: return False\n",
    "        \n",
    "        # Compare with ground truth and update metrics\n",
    "        miss, exs, gts = self._cmp_missing(extracted, gt)\n",
    "        self._update(exs, gts,\n",
    "            _norm(extracted.get(\"name\")) == _norm(gt.get(\"name\")),\n",
    "            _norm_email(extracted.get(\"email\")) == _norm_email(gt.get(\"email\")))\n",
    "        \n",
    "        # Prepare output object with extracted data and diagnostic info\n",
    "        out_obj = {\"name\":extracted.get(\"name\",\"\"), \"email\":extracted.get(\"email\",\"\"),\n",
    "                   \"skills\":extracted.get(\"skills\",[]), \"missing_block\":miss}\n",
    "        \n",
    "        # Write individual result file\n",
    "        output_file = self.paths[\"output_dir\"] / (Path(fname).stem + \".json\")\n",
    "        output_file.write_text(\n",
    "            json.dumps(out_obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        \n",
    "        # Add to aggregate collection if configured\n",
    "        if self.paths.get(\"aggregate_path\"): \n",
    "            self.aggregate[fname]=out_obj\n",
    "        \n",
    "        # Print status if requested\n",
    "        if self.options[\"print_each\"]: \n",
    "            print(f\"[OK] {fname}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def finalize(self) -> Dict:\n",
    "        \"\"\"Write aggregate file (if configured) and return final evaluation metrics.\"\"\"\n",
    "        # Write aggregate output file if path is configured\n",
    "        if self.paths.get(\"aggregate_path\"):\n",
    "            self.paths[\"aggregate_path\"].write_text(\n",
    "                json.dumps(self.aggregate, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        \n",
    "        # Calculate precision, recall, F1 for skills (micro-averaged)\n",
    "        prec = self.tp/(self.tp+self.fp) if (self.tp+self.fp) else 0.0  # TP/(TP+FP)\n",
    "        rec  = self.tp/(self.tp+self.fn) if (self.tp+self.fn) else 0.0  # TP/(TP+FN)\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0  # Harmonic mean\n",
    "        \n",
    "        return {\n",
    "            \"name_accuracy\": round(self.ok_name/self.n_name,4) if self.n_name else 0.0,\n",
    "            \"email_accuracy\": round(self.ok_email/self.n_email,4) if self.n_email else 0.0,\n",
    "            \"skills_precision_micro\": round(prec,4),\n",
    "            \"skills_recall_micro\": round(rec,4),\n",
    "            \"skills_f1_micro\": round(f1,4),\n",
    "            \"num_evaluated\": self.n_name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb166a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] resume_alice.pdf\n",
      "[OK] resume_brian.pdf\n",
      "[OK] resume_carmen.pdf\n",
      "{\n",
      "  \"name_accuracy\": 1.0,\n",
      "  \"email_accuracy\": 1.0,\n",
      "  \"skills_precision_micro\": 0.0,\n",
      "  \"skills_recall_micro\": 0.0,\n",
      "  \"skills_f1_micro\": 0.0,\n",
      "  \"num_evaluated\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ===== runner (no duplicated config usage) =====\n",
    "def main(argv=None):\n",
    "    \"\"\"Main entry point for LLM-based resume extraction pipeline.\"\"\"\n",
    "    # Set up command-line argument parsing\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--config\", default=\"config.yaml\", help=\"Path to YAML configuration file\")\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "\n",
    "    # Load configuration once and pass to all components\n",
    "    cfg = load_cfg(args.config)                              # ← load ONCE\n",
    "    extractor = LLMResumeExtractor(cfg)                      # ← pass cfg\n",
    "    evaluator = ResumeEvaluator(cfg)                         # ← pass cfg\n",
    "\n",
    "    # Extract file processing settings from config\n",
    "    exts = cfg[\"options\"][\"exts\"]; input_dir = cfg[\"paths\"][\"input_dir\"]\n",
    "    \n",
    "    # Process all matching files in input directory\n",
    "    wrote=False\n",
    "    for p in sorted(input_dir.glob(\"*\")):\n",
    "        # Skip files with unsupported extensions\n",
    "        if p.suffix.lower() not in exts: continue\n",
    "        \n",
    "        # Extract data using LLM and evaluate against ground truth\n",
    "        ex = extractor.parse_one(str(p))\n",
    "        wrote |= evaluator.process_one(str(p), ex)  # Track if any files were processed\n",
    "\n",
    "    # Print final evaluation metrics\n",
    "    print(json.dumps(evaluator.finalize(), indent=2))\n",
    "    \n",
    "    # Warn if no files were processed (likely ground truth mismatch)\n",
    "    if not wrote: \n",
    "        print(\"[Note] No files were written (no filename matched ground truth).\")\n",
    "\n",
    "def run_from_notebook(config_path=\"config.yaml\"):\n",
    "    \"\"\"Convenience function for running from Jupyter notebooks.\"\"\"\n",
    "    # Call main with config argument to avoid sys.argv issues in notebooks\n",
    "    main([\"--config\", config_path])\n",
    "\n",
    "# Standard Python script entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f886b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume_Parser_Git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
